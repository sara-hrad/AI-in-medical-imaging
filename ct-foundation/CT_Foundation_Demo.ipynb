{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sara-hrad/AI-in-medical-imaging/blob/main/ct-foundation/CT_Foundation_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjl9khpJ8CoN"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-health/imaging-research/blob/master/ct-foundation/CT_Foundation_Demo.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google-health/imaging-research/tree/master/ct-foundation\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2SSjj4fqzeOe"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnwIgD1bziUF"
      },
      "source": [
        "## CT Foundation API Demo\n",
        "The ipynb is a demonstration of using the\n",
        "[CT Foundation API](https://github.com/google-health/imaging-research/tree/master/ct-foundation)\n",
        "(this API computes embeddings for CT DICOMs).\n",
        "\n",
        "The contents include how to:\n",
        "\n",
        "-   Load the LIDC dataset from DICOMs stored in Google DICOM Store and labels stored in GCS\n",
        "-   Generate embeddings for the image files\n",
        "-   Train a small model using the embeddings\n",
        "\n",
        "**Note**: It can take some time to generate embeddings for thousands of images.\n",
        "For ease of use, by default, this colab uses precomputed embeddings. You can\n",
        "also calculate them from scratch again by updating the relevant param in the\n",
        "\"Global params\" section.\n",
        "\n",
        "### This notebook is for API demonstration purposes only\n",
        "\n",
        "**Note: This notebook is for API demonstration purposes only.**\n",
        "\n",
        "It's important to use evaluation datasets\n",
        "that reflect the expected distribution of images and patients you wish to use any downstream models on.\n",
        "\n",
        "This means that the best way to determine if this API is right for you is to try it with data that would be used for the downstream task you're interested in.\n",
        "\n",
        "**Note**: If you want to jump to training a model with embeddings, you can\n",
        "scroll down to [Train a model with the embeddings from NLST](#train-nlst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awoN_r8jOj0r"
      },
      "source": [
        "# Data Attribution\n",
        "\n",
        "This notebook makes use of two public datasets provided by the Cancer Imaging Archive which is managed by the United States  National Cancer Institute\n",
        "\n",
        "###  NLST Radiology CT Images CC BY 4.0\n",
        "[https://www.cancerimagingarchive.net/collection/nlst/](https://www.cancerimagingarchive.net/collection/nlst/)\n",
        "\n",
        "#### NLST Data Citation\n",
        " National Lung Screening Trial Research Team. (2013). Data from the National Lung Screening Trial (NLST) [Data set]. The Cancer Imaging Archive. https://doi.org/10.7937/TCIA.HMQ8-J677\n",
        "### LIDC-IDRI Data Access CC BY 3.0\n",
        "https://www.cancerimagingarchive.net/collection/lidc-idri/\n",
        "\n",
        "#### LIDC-IDRI Data Citation\n",
        "\n",
        "Armato III, S. G., McLennan, G., Bidaut, L., McNitt-Gray, M. F., Meyer, C. R., Reeves, A. P., Zhao, B., Aberle, D. R., Henschke, C. I., Hoffman, E. A., Kazerooni, E. A., MacMahon, H., Van Beek, E. J. R., Yankelevitz, D., Biancardi, A. M., Bland, P. H., Brown, M. S., Engelmann, R. M., Laderach, G. E., Max, D., Pais, R. C. , Qing, D. P. Y. , Roberts, R. Y., Smith, A. R., Starkey, A., Batra, P., Caligiuri, P., Farooqi, A., Gladish, G. W., Jude, C. M., Munden, R. F., Petkovska, I., Quint, L. E., Schwartz, L. H., Sundaram, B., Dodd, L. E., Fenimore, C., Gur, D., Petrick, N., Freymann, J., Kirby, J., Hughes, B., Casteele, A. V., Gupte, S., Sallam, M., Heath, M. D., Kuhn, M. H., Dharaiya, E., Burns, R., Fryd, D. S., Salganicoff, M., Anand, V., Shreter, U., Vastagh, S., Croft, B. Y., Clarke, L. P. (2015). Data From LIDC-IDRI [Data set]. The Cancer Imaging Archive. https://doi.org/10.7937/K9/TCIA.2015.LO9QL9SX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTpU6d_WPXx8"
      },
      "source": [
        "# Installation & Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfnWmzMCOwzG"
      },
      "outputs": [],
      "source": [
        "# Notebook specific dependencies\n",
        "# !pip install google_health.ct_dicom\n",
        "# TODO OPTIONAL: Create pip installation for the code below - https://github.com/google-health/google-health/tree/master/ct_dicom\n",
        "\n",
        "!pip install absl-py dicomweb-client[gcp] google-auth requests-toolbelt numpy==1.25.0 pandas==2.2.2\n",
        "!pip install tf-models-official==2.14.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "id": "bFabz5dNkZ1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYMxVPMPzpR7"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import http\n",
        "import matplotlib\n",
        "import pydicom\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from typing import Iterable, Optional\n",
        "from google.colab import auth\n",
        "from google.oauth2 import credentials\n",
        "import pandas as pd\n",
        "import dicomweb_client.ext.gcp.uri as gcp_uri\n",
        "import dicomweb_client.uri as dicomweb_uri\n",
        "from google.colab import auth\n",
        "from google.oauth2 import credentials\n",
        "from google.auth import credentials as gcredentials\n",
        "from google.auth.transport import requests\n",
        "from requests_toolbelt.multipart import decoder\n",
        "from google.cloud import storage\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5_an_XYPdBA"
      },
      "source": [
        "**IMPORTANT**: If you are using Colab, you must restart the runtime after installing new packages.\n",
        "\n",
        "NOTE: There will be some ERROR messages due to the protobuf library - this is normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7sY6uzinTGjg"
      },
      "outputs": [],
      "source": [
        "#@title Classes for testing data access and visualization\n",
        "\n",
        "\"\"\"Google Cloud Healthcare (CHC) DICOMweb utilities.\"\"\"\n",
        "\n",
        "# Well-known constants from https://www.dicomstandard.org/.\n",
        "_STUDY_INSTANCE_UID_TAG = '0020000D'\n",
        "_SERIES_INSTANCE_UID_TAG = '0020000E'\n",
        "_SOP_INSTANCE_UID_TAG = '00080018'\n",
        "\n",
        "_SERIES_INSTANCE_UID_SEARCH_SUFFIX = 'series'\n",
        "_STUDY_INSTANCE_UID_SEARCH_SUFFIX = 'studies'\n",
        "_SOP_INSTANCE_UID_SEARCH_SUFFIX = 'instances'\n",
        "\n",
        "_VALUE_KEY = 'Value'\n",
        "\n",
        "# Scope requirements from:\n",
        "# https://cloud.google.com/healthcare-api/docs/reference/rest/v1/projects.locations.datasets.dicomStores/searchForInstances#authorization-scopes\n",
        "_AUTHORIZATION_SCOPES = ['https://www.googleapis.com/auth/cloud-healthcare']\n",
        "\n",
        "# Search result limits for the CHC DICOMweb API:\n",
        "# https://cloud.google.com/healthcare-api/docs/dicom#search_parameters\n",
        "_MAX_LIMIT_STUDY = 5000\n",
        "_MAX_LIMIT_SERIES = 5000\n",
        "_MAX_LIMIT_SOP = 50000\n",
        "_MAX_OFFSET = 1000000\n",
        "\n",
        "_MAX_REFRESH_ATTEMPTS = 10\n",
        "_REQUEST_TIMEOUT_SECONDS = 600\n",
        "\n",
        "\n",
        "def create_authorized_session(\n",
        "    credentials: gcredentials.Credentials,\n",
        ") -> requests.AuthorizedSession:\n",
        "  \"\"\"Creates a Session authorized for Cloud Healthcare API interactions.\n",
        "\n",
        "  Args:\n",
        "    credentials: Google Auth credentials. For further details, see\n",
        "      https://googleapis.dev/python/google-auth/latest/index.html.\n",
        "\n",
        "  Returns:\n",
        "    Credentials object with the requisite API scope.\n",
        "  \"\"\"\n",
        "  authorization_scopes = _AUTHORIZATION_SCOPES\n",
        "  scoped_credentials = gcredentials.with_scopes_if_required(\n",
        "      credentials, authorization_scopes\n",
        "  )\n",
        "  return requests.AuthorizedSession(\n",
        "      scoped_credentials, max_refresh_attempts=_MAX_REFRESH_ATTEMPTS\n",
        "  )\n",
        "\n",
        "\n",
        "def download_multipart_dicom_series(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    dataset_id: str,\n",
        "    dicom_store_id: str,\n",
        "    session: requests.AuthorizedSession,\n",
        "    study_instance_uid: str,\n",
        "    series_instance_uid: str,\n",
        ") -> Iterable[bytes]:\n",
        "  \"\"\"Downloads all SOP Instances (DICOMs) within a Series Instance UID.\n",
        "\n",
        "  The request accepts a multipart MIME response from the CHC DICOMweb API to\n",
        "  reduce the:\n",
        "  - Latency associated with making one API call per Instance.\n",
        "  - API quota usage while downloading all Instances within a Series.\n",
        "\n",
        "  Args:\n",
        "    project_id: The GCP Project containing the DICOM Store to query.\n",
        "    location: The regional location associated with the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/regions).\n",
        "    dataset_id: The Dataset containing the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "    dicom_store_id: The DICOM Store to query.\n",
        "    session: An Google Auth session authorized to use the CHC DICOMweb API.\n",
        "    study_instance_uid: The Study Instance UID containing the Series Instance\n",
        "      UID to download.\n",
        "    series_instance_uid: The Series Instance UID containing the SOP Instances\n",
        "      (DICOMs) to download.\n",
        "\n",
        "  Yields:\n",
        "    DICOM bytes associated with each Instance contained within the input Series\n",
        "    Instance UID.\n",
        "  \"\"\"\n",
        "  dicomweb_path = str(\n",
        "      dicomweb_uri.URI(\n",
        "          str(\n",
        "              gcp_uri.GoogleCloudHealthcareURL(\n",
        "                  project_id, location, dataset_id, dicom_store_id\n",
        "              )\n",
        "          ),\n",
        "          study_instance_uid,\n",
        "          series_instance_uid,\n",
        "      )\n",
        "  )\n",
        "\n",
        "  headers = {\n",
        "      'Accept': (\n",
        "          'multipart/related; transfer-syntax=1.2.840.10008.1.2.1;'\n",
        "          ' type=\"application/dicom\"'\n",
        "      )\n",
        "  }\n",
        "  response = session.get(\n",
        "      dicomweb_path, headers=headers, timeout=_REQUEST_TIMEOUT_SECONDS\n",
        "  )\n",
        "  response.raise_for_status()\n",
        "\n",
        "  for part in decoder.MultipartDecoder.from_response(response).parts:\n",
        "    yield part.content\n",
        "\n",
        "\n",
        "def search_study_instance_uids(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    dataset_id: str,\n",
        "    dicom_store_id: str,\n",
        "    session: requests.AuthorizedSession,\n",
        "    limit: int = 100,\n",
        ") -> Iterable[str]:\n",
        "  \"\"\"Recovers all Study Instance UIDs from a CHC DICOM Store.\n",
        "\n",
        "  Args:\n",
        "    project_id: The GCP Project containing the DICOM Store to query.\n",
        "    location: The regional location associated with the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/regions).\n",
        "    dataset_id: The Dataset containing the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "    dicom_store_id: The DICOM Store to query.\n",
        "    session: An Google Auth session authorized to use the CHC DICOMweb API.\n",
        "    limit: The number of Study Instance UIDs in the DICOM Store could be large.\n",
        "      The UIDs are recovered in a paginated fashion, where each page of results\n",
        "      (one page per query) includes at most `limit` values. The higher this\n",
        "      value, the fewer the total number of requests, but each response would be\n",
        "      larger. Depending on your network connection, set this value in the range\n",
        "      1 through 5000 (both inclusive). This parameter impacts the speed and\n",
        "      network bandwidth utilization, but not the values returned by the method.\n",
        "\n",
        "  Yields:\n",
        "    Study Instance UIDs from the DICOM Store.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If `limit` exceeds the max value of 5000 allowed by the CHC\n",
        "      DICOMweb API (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "  \"\"\"\n",
        "  if limit > _MAX_LIMIT_STUDY:\n",
        "    raise ValueError(\n",
        "        f'Request limit {limit} exceeds the CHC Search query request limit of'\n",
        "        f' {_MAX_LIMIT_STUDY} for Study Instances.'\n",
        "    )\n",
        "  yield from _search_dicom_data(\n",
        "      project_id,\n",
        "      location,\n",
        "      dataset_id,\n",
        "      dicom_store_id,\n",
        "      _STUDY_INSTANCE_UID_SEARCH_SUFFIX,\n",
        "      _STUDY_INSTANCE_UID_TAG,\n",
        "      session,\n",
        "      limit,\n",
        "  )\n",
        "\n",
        "\n",
        "def search_series_instance_uids(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    dataset_id: str,\n",
        "    dicom_store_id: str,\n",
        "    session: requests.AuthorizedSession,\n",
        "    study_instance_uid: Optional[str] = None,\n",
        "    limit: int = 100,\n",
        ") -> Iterable[str]:\n",
        "  \"\"\"Recovers all Series Instance UIDs from a CHC DICOM Store.\n",
        "\n",
        "  The scope may be restricted to all Series within a fixed Study Instance\n",
        "  UIDs (see `study_instance_uid` below).\n",
        "\n",
        "  Args:\n",
        "    project_id: The GCP Project containing the DICOM Store to query.\n",
        "    location: The regional location associated with the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/regions).\n",
        "    dataset_id: The Dataset containing the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "    dicom_store_id: The DICOM Store to query.\n",
        "    session: An Google Auth session authorized to use the CHC DICOMweb API.\n",
        "    study_instance_uid: If provided, restricts the returned Series Instance UIDs\n",
        "      to within this Study Instance UID.\n",
        "    limit: The number of Study Instance UIDs in the DICOM Store could be large.\n",
        "      The UIDs are recovered in a paginated fashion, where each page (query)\n",
        "      includes at most `limit` values. The higher this value, the fewer the\n",
        "      total number of requests, but each response would be larger. Depending on\n",
        "      your network connection, set this value in the range 1 through 5000 (both\n",
        "      inclusive).\n",
        "\n",
        "  Yields:\n",
        "    Series Instance UIDs from the DICOM Store (optionally within the scope of\n",
        "    the input `study_instance_uid`, if provided).\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If `limit` exceeds the max value of 5000 allowed by the CHC\n",
        "      DICOMweb API (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "  \"\"\"\n",
        "  if limit > _MAX_LIMIT_SERIES:\n",
        "    raise ValueError(\n",
        "        f'Request limit {limit} exceeds the CHC Search query request limit of'\n",
        "        f' {_MAX_LIMIT_SERIES} for Series Instances.'\n",
        "    )\n",
        "  search_suffix = (\n",
        "      _SERIES_INSTANCE_UID_SEARCH_SUFFIX\n",
        "      if study_instance_uid is None\n",
        "      else f'studies/{study_instance_uid}/series'\n",
        "  )\n",
        "  yield from _search_dicom_data(\n",
        "      project_id,\n",
        "      location,\n",
        "      dataset_id,\n",
        "      dicom_store_id,\n",
        "      search_suffix,\n",
        "      _SERIES_INSTANCE_UID_TAG,\n",
        "      session,\n",
        "      limit,\n",
        "  )\n",
        "\n",
        "\n",
        "def _search_dicom_data(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    dataset_id: str,\n",
        "    dicom_store_id: str,\n",
        "    query_suffix: str,\n",
        "    dicom_tag: str,\n",
        "    session: requests.AuthorizedSession,\n",
        "    limit: int,\n",
        ") -> Iterable[str]:\n",
        "  \"\"\"Generates DICOM UIDs from a CHC DICOM Store.\"\"\"\n",
        "  assert limit > 0\n",
        "\n",
        "  uri = gcp_uri.GoogleCloudHealthcareURL(\n",
        "      project_id, location, dataset_id, dicom_store_id\n",
        "  )\n",
        "  base_dicomweb_query_path = f'{uri}/{query_suffix}?includefield={dicom_tag}'\n",
        "  headers = {'Content-Type': 'application/dicom+json; charset=utf-8'}\n",
        "\n",
        "  # The CHC offset limit puts an upper bound on the Instance count, which is\n",
        "  # also used to limit the number of iterations.\n",
        "  for offset in range(0, _MAX_OFFSET, limit):\n",
        "    dicomweb_query_path = (\n",
        "        f'{base_dicomweb_query_path}&offset={offset}&limit={limit}'\n",
        "    )\n",
        "\n",
        "    response = session.get(\n",
        "        dicomweb_query_path, headers=headers, timeout=_REQUEST_TIMEOUT_SECONDS\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    # CHC DICOMweb API does not set a Warning response header on the last\n",
        "    # available page:\n",
        "    # https://cloud.google.com/healthcare-api/docs/dicom#search_parameters\n",
        "    if response.status_code == http.HTTPStatus.NO_CONTENT:\n",
        "      return\n",
        "\n",
        "    for instance in response.json():\n",
        "      assert dicom_tag in instance\n",
        "      assert _VALUE_KEY in instance[dicom_tag]\n",
        "\n",
        "      for value in instance[dicom_tag][_VALUE_KEY]:\n",
        "        # if dicomweb_uri._REGEX_UID.fullmatch(value): #Added validation of the UID\n",
        "          yield value\n",
        "        # else:\n",
        "        #   print(f\"Warning: Skipping invalid UID: {value}\") # Added warning message for skipped UIDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb82fhNmYF1P"
      },
      "outputs": [],
      "source": [
        "# @title Authenticate\n",
        "# Authenticate user for access. There will be a popup asking you to sign in with your user and approve access.\n",
        "auth.authenticate_user()\n",
        "TOKEN_ = !gcloud beta auth application-default print-access-token\n",
        "TOKEN = TOKEN_[0]\n",
        "\n",
        "# This is your token for accessing the API and CT Volumes.\n",
        "# It's good for 1 hour until you need a new one.\n",
        "TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lfmFZPCFM2G2"
      },
      "outputs": [],
      "source": [
        "# @title Set DICOM store parameters\n",
        "project_id='ctpe-442718' # @param {type:\"string\"}\n",
        "location='us-central1'  # @param {type:\"string\"}\n",
        "dataset_id='ctpe-dicom-all'  # @param {type:\"string\"}\n",
        "# dicom_store_id='ctpe-dicom-store' # @param {type:\"string\"}\n",
        "# dicom_store_id = 'ctpe_balanced-rest' # @param {type:\"string\"}\n",
        "dicom_store_id = 'ctpe-negative-more' # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JQRxrXiUUgv"
      },
      "source": [
        "<a name=\"train-nlst\"></a>\n",
        "# Train a model with the embeddings from NLST\n",
        "\n",
        "Here we have a full set of embeddings from the NLST dataset that you can download and train a cancer detection model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkB5R0sx7C7Z"
      },
      "source": [
        "## Collect the stored NPZ data from the cloud bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzI0A92hH5VZ"
      },
      "outputs": [],
      "source": [
        "# GCS bucket with data to read:\n",
        "gcs_storage_client = storage.Client(project_id) # our bucket is in the same Google Cloud Project as the DicomStore\n",
        "gcs_bucket_name = 'hai-cd3-foundations-ct3d-vault-entry'# @param {type:\"string\"}\n",
        "gcs_bucket = gcs_storage_client.bucket(gcs_bucket_name)\n",
        "tune_path = 'nlst/nlst_tune_with_labels.npz' # @param {type:\"string\"}\n",
        "train_path = 'nlst/nlst_train_with_labels.npz' # @param {type:\"string\"}\n",
        "\n",
        "def read_embeddings(path):\n",
        "  blob = gcs_bucket.blob(path)\n",
        "  with blob.open('rb') as f:\n",
        "    data = np.load(f,allow_pickle=True)\n",
        "    key = data.files[0]\n",
        "    return pd.DataFrame.from_dict(data[key].item(), orient='index')\n",
        "\n",
        "df_tune = read_embeddings(tune_path)\n",
        "df_train = read_embeddings(train_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o15b23RR7WYz"
      },
      "source": [
        "## Train and Evaluate a model using precomputed embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1cEUPaH90DV"
      },
      "outputs": [],
      "source": [
        "# choose whether you want predict whether the screen leads to a positive lung cancer diagnosis within 1 or 2 years.\n",
        "model_head = 'cancer_in_2' # @param [\"cancer_in_1\",\"cancer_in_2\"]\n",
        "# Get NumPy arrays from DataFrames\n",
        "train_embeddings = df_train.embedding\n",
        "train_labels = df_train[model_head].values\n",
        "tune_embeddings = df_tune.embedding\n",
        "tune_labels = df_tune[model_head].values\n",
        "\n",
        "# Convert the NumPy arrays to ragged tensors\n",
        "train_embeddings = tf.constant(list(train_embeddings))\n",
        "tune_embeddings = tf.convert_to_tensor(list(tune_embeddings))\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_embeddings, train_labels))\n",
        "eval_ds = tf.data.Dataset.from_tensor_slices((tune_embeddings, tune_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnAmHU5mA88c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_models as tfm\n",
        "\n",
        "\n",
        "def create_model(heads,\n",
        "                 token_num=1,\n",
        "                 embeddings_size=1408,\n",
        "                 learning_rate=0.07,\n",
        "                 end_lr_factor=1.0,\n",
        "                 dropout=0.5,\n",
        "                 loss_weights=None,\n",
        "                 hidden_layer_sizes=[128, 32],\n",
        "                 weight_decay=0.0001,\n",
        "                 seed=None) -> tf.keras.Model:\n",
        "  \"\"\"\n",
        "  Creates linear probe or multilayer perceptron using LARS.\n",
        "\n",
        "  \"\"\"\n",
        "  inputs = tf.keras.Input(shape=(token_num * embeddings_size,))\n",
        "  inputs_reshape = tf.keras.layers.Reshape((token_num, embeddings_size))(inputs)\n",
        "  inputs_pooled = tf.keras.layers.GlobalAveragePooling1D(data_format='channels_last')(inputs_reshape)\n",
        "  hidden = inputs_pooled\n",
        "  # If no hidden_layer_sizes are provided, model will be a linear probe.\n",
        "  for size in hidden_layer_sizes:\n",
        "    hidden = tf.keras.layers.Dense(\n",
        "        size,\n",
        "        activation='relu',\n",
        "        kernel_initializer=tf.keras.initializers.HeUniform(seed=seed),\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(l2=weight_decay),\n",
        "        bias_regularizer=tf.keras.regularizers.l2(l2=weight_decay))(\n",
        "            hidden)\n",
        "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
        "    hidden = tf.keras.layers.Dropout(dropout, seed=seed)(hidden)\n",
        "  output = tf.keras.layers.Dense(\n",
        "      units=len(heads),\n",
        "      activation='sigmoid',\n",
        "      kernel_initializer=tf.keras.initializers.HeUniform(seed=seed))(\n",
        "          hidden)\n",
        "\n",
        "  outputs = {}\n",
        "  for i, head in enumerate(heads):\n",
        "    outputs[head] = tf.keras.layers.Lambda(\n",
        "        lambda x: x[..., i:i + 1], name=head.lower())(\n",
        "            output)\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  model.compile(\n",
        "      optimizer=tfm.optimization.lars.LARS(\n",
        "         learning_rate=.1),\n",
        "      loss=dict([(head, 'binary_focal_crossentropy') for head in heads]),\n",
        "      loss_weights=loss_weights or 1.0,\n",
        "      weighted_metrics=[\n",
        "        tf.keras.metrics.FalsePositives(),\n",
        "        tf.keras.metrics.FalseNegatives(),\n",
        "        tf.keras.metrics.TruePositives(),\n",
        "        tf.keras.metrics.TrueNegatives(),\n",
        "        tf.keras.metrics.AUC(),\n",
        "        tf.keras.metrics.AUC(curve='PR', name='auc_pr')])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZIUOHpWEyj5"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "DIAGNOSIS = 'cancer_in_2'\n",
        "model = create_model(\n",
        "    [DIAGNOSIS]\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    x=train_ds.batch(512).prefetch(tf.data.AUTOTUNE).cache(),\n",
        "    validation_data=eval_ds.batch(32).cache(),\n",
        "    epochs=35,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh24XhCaE15V"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_curve(x, y, auc, x_label=None, y_label=None, label=None):\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  plt.plot(x, y, label=f'{label} (AUC: %.3f)' % auc, color='black')\n",
        "  plt.legend(loc='lower right', fontsize=18)\n",
        "  plt.xlim([-0.01, 1.01])\n",
        "  plt.ylim([-0.01, 1.01])\n",
        "  if x_label:\n",
        "    plt.xlabel(x_label, fontsize=24)\n",
        "  if y_label:\n",
        "    plt.ylabel(y_label, fontsize=24)\n",
        "  plt.xticks(fontsize=12)\n",
        "  plt.yticks(fontsize=12)\n",
        "  plt.grid(visible=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-n_8DvClC9Z"
      },
      "outputs": [],
      "source": [
        "rows = []\n",
        "for embeddings, label in eval_ds.batch(1):\n",
        "  row = {\n",
        "      f'{DIAGNOSIS}_prediction': model(embeddings)[DIAGNOSIS].numpy().flatten()[0],\n",
        "      f'{DIAGNOSIS}_value': label.numpy().flatten()[0]\n",
        "  }\n",
        "  rows.append(row)\n",
        "eval_df = pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULHjSMx8lFyY"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "labels = eval_df[f'{DIAGNOSIS}_value'].values\n",
        "predictions = eval_df[f'{DIAGNOSIS}_prediction'].values\n",
        "false_positive_rate, true_positive_rate, thresholds = sklearn.metrics.roc_curve(\n",
        "    labels,\n",
        "    predictions,\n",
        "    drop_intermediate=False)\n",
        "auc = sklearn.metrics.roc_auc_score(labels, predictions)\n",
        "plot_curve(false_positive_rate, true_positive_rate, auc, x_label='False Positive Rate', y_label='True Positive Rate', label=DIAGNOSIS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo_McpwVQRCV"
      },
      "source": [
        "# Test access to LIDC DICOM store and the API\n",
        "\n",
        "Get a token that grants access to the DICOM store and use it to download a volume via the DICOMWEb API. Next, we can collect the embeddings from the\n",
        "selected embedding.\n",
        "\n",
        "**NOTE**: You can skip this section if you just want to train a model on NLST data using the embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szMANNgm4crh"
      },
      "source": [
        "## Download a CT volume to visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6VOuJFzaXlWe"
      },
      "outputs": [],
      "source": [
        "# @title Create a Session via a token.\n",
        "creds = credentials.Credentials(TOKEN)\n",
        "session = create_authorized_session(creds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0qKyk_7ZLfK"
      },
      "outputs": [],
      "source": [
        "# @title List the study and series instance UIDs and download a single volume.\n",
        "\n",
        "# TOTAL_VOLUMES = 378  # Total number to list out.\n",
        "TOTAL_VOLUMES = 500  # Total number to list out.\n",
        "VOLUME_TO_SHOW = 2 # Which ones to render below\n",
        "\n",
        "study_uids = list(search_study_instance_uids(\n",
        "    project_id=project_id,\n",
        "    location=location,\n",
        "    dataset_id=dataset_id,\n",
        "    dicom_store_id=dicom_store_id,\n",
        "    session=session))\n",
        "\n",
        "\n",
        "corresponding_series_uids = []\n",
        "for study_number, a_study_uid in enumerate(study_uids):\n",
        "  a_series = list(search_series_instance_uids(project_id=project_id,\n",
        "    location=location,\n",
        "    dataset_id=dataset_id,\n",
        "    dicom_store_id=dicom_store_id,\n",
        "    session=session, study_instance_uid=a_study_uid))[0]\n",
        "  corresponding_series_uids.append(a_series)\n",
        "  if study_number == TOTAL_VOLUMES:\n",
        "    break\n",
        "\n",
        "\n",
        "volume_as_bytes = list(download_multipart_dicom_series(\n",
        "      project_id=project_id,\n",
        "      location=location,\n",
        "      dataset_id=dataset_id,\n",
        "      dicom_store_id=dicom_store_id,\n",
        "      session=session,\n",
        "      study_instance_uid=study_uids[VOLUME_TO_SHOW],\n",
        "      series_instance_uid=corresponding_series_uids[VOLUME_TO_SHOW],\n",
        "  ))\n",
        "\n",
        "print(f'Total Slices in downloaded volume: {len(volume_as_bytes)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJUE96Qh0ZQY"
      },
      "outputs": [],
      "source": [
        "#@title Render a single slice\n",
        "SLICE_TO_RENDER = 150\n",
        "example_dicom = pydicom.dcmread(io.BytesIO(volume_as_bytes[SLICE_TO_RENDER]))\n",
        "\n",
        "arr_unsigned = example_dicom.pixel_array.copy()\n",
        "arr_unsigned = arr_unsigned.astype(np.float32)\n",
        "arr_unsigned[arr_unsigned <0] = 0\n",
        "arr_unsigned[arr_unsigned >1000] = 1000\n",
        "arr_unsigned = (arr_unsigned / 1000) * 255\n",
        "arr_unsigned = arr_unsigned.astype(np.uint8)\n",
        "Image.fromarray(arr_unsigned, mode='L')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPOseIHV4lAF"
      },
      "source": [
        "## Call the API to compute embeddings for the selected volume.\n",
        "\n",
        "**NOTE:** *The API can take up to 10 minutes to scale individual instances. If you get errors, wait and attempt them again.*\n",
        "\n",
        "Errors results in a FAIL Status string instead of embeddings in the returned list.\n",
        "\n",
        "**NOTE:** Up to 300 parallel requests can be made if the system is fully scaled. Please start at 50 and reduce requests if you are getting end point errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qzal615K4rDv"
      },
      "outputs": [],
      "source": [
        "#@title Python methods to call CT Foundation's API.\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import dataclasses\n",
        "import functools\n",
        "import json\n",
        "from typing import Any, Tuple\n",
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(eq=False, frozen=True)\n",
        "class Response:\n",
        "  \"\"\"Response from a Vertex Endpoint.\"\"\"\n",
        "\n",
        "  status_code: int\n",
        "  response_json: dict[str, Any] | None  # json_types.JSONObject\n",
        "\n",
        "\n",
        "class Endpoint:\n",
        "  \"\"\"Calling utility for a Vertex Endpoint using default credentials.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self._endpoint_url = (\n",
        "        'https://us-central1-aiplatform.googleapis.com/v1/projects/'\n",
        "        'hai-cd3-foundations/locations/us-central1/endpoints/300')\n",
        "\n",
        "  def predict(\n",
        "      self,\n",
        "      instances=list[Any],\n",
        "      parameters: dict[str, Any] | None = None,\n",
        "      credentials: google.auth.credentials.Credentials | None = None,\n",
        "  ) -> Response:\n",
        "    \"\"\"Calls the Vertex Endpoint with the given instances and parameters.\"\"\"\n",
        "    if credentials is None:\n",
        "      credentials = google.auth.default()[0]\n",
        "    session = google.auth.transport.requests.AuthorizedSession(\n",
        "        credentials=credentials\n",
        "    )\n",
        "    response = session.post(\n",
        "        self._endpoint_url + ':predict',\n",
        "        json=(\n",
        "            {'instances': instances}\n",
        "            | ({'parameters': parameters} if parameters is not None else {})\n",
        "        ),\n",
        "        headers={\n",
        "            'Content-Type': 'application/json',\n",
        "        },\n",
        "        timeout=400\n",
        "    )\n",
        "    try:\n",
        "      response_json = response.json()\n",
        "    except json.JSONDecodeError:\n",
        "      # Not expected, handling in case server incorrectly returns non-JSON.\n",
        "      response_json = None\n",
        "    return Response(\n",
        "        status_code=response.status_code,\n",
        "        response_json=response_json,\n",
        "    )\n",
        "\n",
        "\n",
        "def call_single_batch(\n",
        "    caller: Endpoint,\n",
        "    credentials,\n",
        "    urls: list[str],\n",
        "    access_token: str\n",
        ") -> list[Tuple[np.ndarray | str, str]]:\n",
        "  \"\"\"Handles calls for a single batch and returns embeddings.\"\"\"\n",
        "  return_data = []\n",
        "  if not credentials.valid:\n",
        "    credentials.refresh(google.auth.transport.requests.Request())\n",
        "  instances = [{\n",
        "      \"dicom_path\": a_url, \"bearer_token\": f\"{access_token}\"} for a_url in urls]\n",
        "  returns = caller.predict(instances=instances)\n",
        "  if returns.status_code != 200:\n",
        "    for a_url in urls:\n",
        "      return_data.append((f'FAIL STATUS {returns.status_code}', a_url))\n",
        "    return return_data\n",
        "  else:\n",
        "    for i in range(len(returns.response_json['predictions'])):\n",
        "      if returns.response_json['predictions'][i]['error_response']:\n",
        "        return_data.append((\n",
        "            returns.response_json['predictions'][i]['error_response'], urls[i]))\n",
        "      else:\n",
        "        embeddings = returns.response_json['predictions'][i][\n",
        "            'embedding_result'\n",
        "        ]['embedding']\n",
        "        return_data.append((embeddings, urls[i]))\n",
        "    return return_data\n",
        "\n",
        "\n",
        "def get_ct_embbeddings(\n",
        "    caller: Endpoint,\n",
        "    credentials,\n",
        "    urls: list[str],\n",
        "    access_token: str,\n",
        "    batch_size: int,\n",
        "    parallel_size: int,\n",
        ") -> list[Tuple[np.ndarray | str, str]]:\n",
        "  \"\"\"Handles calls and returns for parallel requests.\n",
        "\n",
        "  Args:\n",
        "    caller: CT foundation API caller.\n",
        "    credentials: The credentials for the API.\n",
        "    urls: List of urls to the DICOM store for series to run.\n",
        "      This must be of length batch_size * parallel_size.\n",
        "    access_token: Access token for the DICOM store.\n",
        "    batch_size: The number of volumes to pass in a batch (max 5).\n",
        "    parallel_size: The number of parallel calls.\n",
        "\n",
        "  Returns:\n",
        "    Tuple list of embeddings | errors and the corresponding urls from which\n",
        "      the embeddings were computed.\n",
        "  \"\"\"\n",
        "  assert batch_size < 6, 'Batch size must be 5 or less.'\n",
        "  assert (\n",
        "      len(urls) == batch_size * parallel_size\n",
        "  ), 'Error in batch, parallel sizes versus requests'\n",
        "\n",
        "  # Setup up parallel batches\n",
        "  p_urls = []\n",
        "  for i in range(parallel_size):\n",
        "    p_urls.append(urls[i * batch_size : (i + 1) * batch_size])\n",
        "\n",
        "  # Check for correct sizing\n",
        "  assert len(p_urls) == parallel_size, 'Error in batch, parallel dimensions'\n",
        "\n",
        "  call_batch = functools.partial(call_single_batch, caller, credentials)\n",
        "\n",
        "  # Launch parallel calls\n",
        "  with ThreadPoolExecutor(max_workers=parallel_size) as executor:\n",
        "    futures = [\n",
        "        executor.submit(call_batch, b_urls, access_token) for b_urls in p_urls\n",
        "    ]\n",
        "    results = [f.result() for f in futures]\n",
        "  # Unpack results into a single list\n",
        "  return_results = []\n",
        "  for b_result in results:\n",
        "    for a_result in b_result:\n",
        "      return_results.append(a_result)\n",
        "  return return_results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df_no_embed = pd.read_csv('series_false_embed.csv')\n",
        "df_no_embed = pd.read_csv('series_remaining.csv')\n",
        "\n",
        "num_series = len(corresponding_series_uids)\n",
        "df_list_uids = pd.DataFrame()\n",
        "print(num_series)\n",
        "for volume in range(num_series):\n",
        "  # The url pointing to the specific DICOM series that can be accessed via\n",
        "  # the above token.\n",
        "  df_list_uids.loc[volume, 'study_id'] = study_uids[volume]\n",
        "  df_list_uids.loc[volume, 'series_id'] = corresponding_series_uids[volume]\n",
        "\n",
        "merged_df = pd.merge(df_no_embed,df_list_uids, on='series_id', how='left')\n",
        "series_list =merged_df['series_id'].values\n",
        "common = list(set(series_list) & set(df_no_embed['series_id'].values))\n",
        "studies_list = merged_df['study_id'].values\n",
        "print(len(common))\n",
        "print(merged_df.head())\n"
      ],
      "metadata": {
        "id": "oMxPxFh6XHJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_YHl2E30nHo"
      },
      "outputs": [],
      "source": [
        "#@title Create a URL, token, and call the API for the DICOM volume\n",
        "\n",
        "def create_lidc_series_url(study_instance_uid, series_instance_uid):\n",
        "  \"\"\"Create a URL to the specific LIDC DICOM volume.\"\"\"\n",
        "  return ('https://healthcare.googleapis.com/v1/projects/ctpe-442718/'\n",
        "          'locations/us-central1/datasets/ctpe-dicom-all/dicomStores/ctpe-negative-more/dicomWeb/'\n",
        "          f'studies/{study_instance_uid}/series/{series_instance_uid}')\n",
        "  # return ('https://healthcare.googleapis.com/v1/projects/ctpe-442718/'\n",
        "  #         'locations/us-central1/datasets/ctpe-dicom-all/dicomStores/ctpe_balanced-rest/dicomWeb/'\n",
        "  #         f'studies/{study_instance_uid}/series/{series_instance_uid}')\n",
        "  # return ('https://healthcare.googleapis.com/v1/projects/ctpe-442718/'\n",
        "  #         'locations/us-central1/datasets/ctpe-dicom-all/dicomStores/ctpe-dicom-store/dicomWeb/'\n",
        "  #         f'studies/{study_instance_uid}/series/{series_instance_uid}')\n",
        "\n",
        "# Credentials to access the API\n",
        "credentials = google.auth.default()[0]\n",
        "\n",
        "# Token to access the DICOMs in the DICOM store\n",
        "TOKEN_ = !gcloud beta auth application-default print-access-token\n",
        "TOKEN = TOKEN_[0]\n",
        "\n",
        "# num_series = list(range(len(corresponding_series_uids)))\n",
        "start_id = 160\n",
        "end_id = 240\n",
        "num_series = list(range(start_id,end_id))\n",
        "# num_series = list(range(len(series_list)))\n",
        "b_size = 4\n",
        "p_size = 20\n",
        "url_list = []\n",
        "\n",
        "for volume in num_series:\n",
        "  # The url pointing to the specific DICOM series that can be accessed via\n",
        "  # the above token.\n",
        "  # my_url = create_lidc_series_url(studies_list[volume],\n",
        "  #                                 series_list[volume])\n",
        "  my_url = create_lidc_series_url(study_uids[volume],\n",
        "                                  corresponding_series_uids[volume])\n",
        "  url_list.append(my_url)\n",
        "\n",
        "\n",
        "# # The url pointing to the specific DICOM series that can be accessed via\n",
        "# # the above token.\n",
        "# my_url = create_lidc_series_url(study_uids[VOLUME_TO_SHOW],\n",
        "#                                 corresponding_series_uids[VOLUME_TO_SHOW])\n",
        "\n",
        "# # Call the API with a single call and a batch size of 1.\n",
        "# my_embeddings = get_ct_embbeddings(\n",
        "#     caller=Endpoint(), credentials=credentials, urls=[my_url],\n",
        "#     access_token=TOKEN, batch_size=1, parallel_size=1)\n",
        "\n",
        "# Call the API with several single call and a batch size of 5.\n",
        "# embedding_list = []\n",
        "# for i in range(3):\n",
        "#   my_embeddings = get_ct_embbeddings(\n",
        "#     caller=Endpoint(), credentials=credentials, urls=url_list[i*100:(i+1)*100],\n",
        "#     access_token=TOKEN, batch_size=5, parallel_size=20)\n",
        "#   for volume in range(100):\n",
        "#     embedding_numpy = np.array(my_embeddings[volume][0])\n",
        "#     embedding_list.append(embedding_numpy)\n",
        "\n",
        "\n",
        "series_id = [corresponding_series_uids[i] for i in num_series]\n",
        "my_embeddings = get_ct_embbeddings(\n",
        "    caller=Endpoint(), credentials=credentials, urls=url_list,\n",
        "    access_token=TOKEN, batch_size=b_size, parallel_size=p_size)\n",
        "print(f'Embeddings or error message for the CT in the DICOM store at: {my_embeddings[0][1]}')\n",
        "print(my_embeddings[0][0])\n",
        "# new_embeddings = [[my_embeddings[i][0], series_list[i]] for i in range(len(num_series))]\n",
        "new_embeddings = [[my_embeddings[i][0], series_id[i]] for i in range(len(num_series))]\n",
        "df_whole = pd.DataFrame(new_embeddings)\n",
        "# name = f'embedding_{start_id}_{end_id}.csv'\n",
        "# name = 'embedding_rest.csv'\n",
        "# name = 'embedding_remaining.csv'\n",
        "name = f'embedding_moredata_{start_id}_{end_id}.csv'\n",
        "df_whole.to_csv(name,  index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_series)\n",
        "series_id = [corresponding_series_uids[i] for i in num_series]\n",
        "new_embeddings = [[my_embeddings[i][0], series_id[i]] for i in range(len(num_series))]\n",
        "name = f'embedding_moredata_{start_id}_{end_id}.csv'\n",
        "df_whole.to_csv(name,  index=False)"
      ],
      "metadata": {
        "id": "1j3RkFlfqWKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(embedding_list[0:-1][:]))\n",
        "series_id = [corresponding_series_uids[i] for i in num_series]\n",
        "print(len(num_series))\n",
        "df_embeddings = pd.DataFrame({'embedding': embedding_list[0:-1][:], 'series_id': series_id})\n",
        "df_whole = pd.DataFrame(my_embeddings)\n",
        "df_whole.to_csv('whole_embedding.csv',  index=False)\n",
        "print(len(df_embeddings['embedding'].values[0]))\n",
        "df_embeddings.to_csv('embeddings.csv',  index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wd94gLOWbS6",
        "outputId": "97f54583-ab60-422a-8619-86b26db323a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "100\n",
            "1408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cx9ZvHRysS3"
      },
      "source": [
        "# Trying CT Foundation on your own CT DICOMs\n",
        "\n",
        "\n",
        "1.   [Create your own DICOM store](https://cloud.google.com/healthcare-api/docs/)\n",
        "2.   Upload your DICOMs to the store.\n",
        "3.   Call the API for a given study / series in your DICOM store.\n",
        "4.   Collect and store your embeddings for training.\n",
        "\n",
        "**NOTE**: If performing parallel calls, i.e. parallel_size >1, please start at\n",
        "50 or less as a start.\n",
        "\n",
        "If you have any feedback or questions please email us at: ct-foundation@google.com\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KTpU6d_WPXx8"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}